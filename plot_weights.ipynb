{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.colorbar import colorbar\n",
    "\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder name\n",
    "EXPERIMENT = 'experiment_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = torch.load(EXPERIMENT + '/train_1.params')\n",
    "e2 = torch.load(EXPERIMENT + '/train_2.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0891973ff45a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bone'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'equal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axarr[{}]: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/deep-learning/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/deep-learning/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5122\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5124\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5125\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.2/envs/deep-learning/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    598\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[1;32m    599\u001b[0m                 (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    }
   ],
   "source": [
    "# vmin and vmax are used in conjunction with norm to normalize luminance data. Note if you pass a norm instance, your settings for vmin and vmax will be ignored.\n",
    "vmin = -1\n",
    "vmax = 1\n",
    "\n",
    "\n",
    "params = list(e1)\n",
    "num_axes = len(params) # + 1  # one extra for the colorbar\n",
    "f, axarr = plt.subplots(ncols=num_axes, sharey=True)\n",
    "for i, param in enumerate(params):\n",
    "    index = i # + 1  # colorbar is at position 0\n",
    "    if len(param.size()) == 1:\n",
    "        param = param.unsqueeze(1)\n",
    "    image = axarr[index].imshow(param.data.numpy(), cmap='bone', aspect='equal', vmin=vmin, vmax=vmax, extent=(0, 5, 0, 1))\n",
    "    axarr[index].set_title('axarr[{}]: {}'.format(index, param.size()))\n",
    "\n",
    "# grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "#              nrows_ncols=(1, 3),\n",
    "#              axes_pad=0.1,  # spacing between things\n",
    "#              add_all=True,\n",
    "#              label_mode=\"L\",\n",
    "#              )\n",
    "\n",
    "# grid2 = ImageGrid(F, 212,\n",
    "#       nrows_ncols = (1, 3),\n",
    "#       direction=\"row\",\n",
    "#       axes_pad = 0.05,\n",
    "#       add_all=True,\n",
    "#       label_mode = \"1\",\n",
    "#       share_all = True,\n",
    "#       cbar_location=\"right\",\n",
    "#       cbar_mode=\"single\",\n",
    "#       cbar_size=\"10%\",\n",
    "#       cbar_pad=0.05,\n",
    "#       )\n",
    "\n",
    "# f.subplots_adjust(hspace=30)\n",
    "\n",
    "# cb1 = f.colorbar(image, ax=axarr[0])\n",
    "# cb1 = f.colorbar(image, ax=axarr[0], height=\"100%\")  # __init__ got unexpected kwarg 'height'\n",
    "\n",
    "plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "plt.show()\n",
    "\n",
    "# draw_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTEMPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageGrid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f83e4aac48c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     axarr[index].set_title('axarr[{}]: {}'.format(index, param.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m grid = ImageGrid(f, 111,  # similar to subplot(111)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mnrows_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0maxes_pad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# spacing between things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageGrid' is not defined"
     ]
    }
   ],
   "source": [
    "# vmin and vmax are used in conjunction with norm to normalize luminance data. Note if you pass a norm instance, your settings for vmin and vmax will be ignored.\n",
    "vmin = -1\n",
    "vmax = 1\n",
    "\n",
    "\n",
    "params = list(e1)\n",
    "num_cols = len(params) # + 1  # one extra for the colorbar\n",
    "f, axarr = plt.subplots(ncols=num_cols, sharey=True)\n",
    "# for i, param in enumerate(params):\n",
    "#     index = i # + 1  # colorbar is at position 0\n",
    "#     if len(param.size()) == 1:\n",
    "#         param = param.unsqueeze(1)\n",
    "#     image = axarr[index].imshow(param.data.numpy(), cmap='bone', aspect='equal', vmin=vmin, vmax=vmax, extent=(0, 5, 0, 1))\n",
    "#     axarr[index].set_title('axarr[{}]: {}'.format(index, param.size()))\n",
    "\n",
    "grid = ImageGrid(f, 111,  # similar to subplot(111)\n",
    "    nrows_ncols=(1, num_cols),\n",
    "    axes_pad=0.1,  # spacing between things\n",
    "    add_all=True,\n",
    "    label_mode=\"L\",\n",
    ")\n",
    "\n",
    "for ax, param in zip(grid, params):\n",
    "    im = ax.imshow(param,\n",
    "                   origin=\"lower\",\n",
    "                   interpolation=\"nearest\")\n",
    "\n",
    "plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "plt.show()\n",
    "\n",
    "# draw_net(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAADFCAYAAABevum5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wPJXLIoBjJ/2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106ba29e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "  \n",
    "def get_demo_image():\n",
    "    from matplotlib.cbook import get_sample_data\n",
    "    f = get_sample_data(\"axes_grid/bivariate_normal.npy\", asfileobj=False)\n",
    "    z = np.load(f)\n",
    "    # z is a numpy array of 15x15\n",
    "    return z, (-3,4,-4,3)\n",
    "  \n",
    "  \n",
    "def add_inner_title(ax, title, loc, size=None, **kwargs):\n",
    "    from matplotlib.offsetbox import AnchoredText\n",
    "    from matplotlib.patheffects import withStroke\n",
    "    if size is None:\n",
    "        size = dict(size=plt.rcParams['legend.fontsize'])\n",
    "    at = AnchoredText(title, loc=loc, prop=size,\n",
    "                      pad=0., borderpad=0.5,\n",
    "                      frameon=False, **kwargs)\n",
    "    ax.add_artist(at)\n",
    "    at.txt._text.set_path_effects([withStroke(foreground=\"w\", linewidth=3)])\n",
    "    return at\n",
    "  \n",
    "if 1:\n",
    "    F = plt.figure(1, (6, 6))\n",
    "    F.clf()t\n",
    "  \n",
    "    # prepare images\n",
    "    Z, extent = get_demo_image()\n",
    "    ZS = [Z[i::3,:] for i in range(3)]\n",
    "    extent = extent[0], extent[1]/3., extent[2], extent[3]\n",
    "  \n",
    "    grid2 = ImageGrid(F, 212,\n",
    "                      nrows_ncols = (1, 3),\n",
    "                      direction=\"row\",\n",
    "                      axes_pad = 0.05,\n",
    "                      add_all=True,\n",
    "                      label_mode = \"1\",\n",
    "                      share_all = True,\n",
    "                      cbar_location=\"right\",\n",
    "                      cbar_mode=\"single\",\n",
    "                      cbar_size=\"10%\",\n",
    "                      cbar_pad=0.05,\n",
    "                      )\n",
    "  \n",
    "    grid2[0].set_xlabel(\"X\")\n",
    "    grid2[0].set_ylabel(\"Y\")\n",
    "  \n",
    "    vmax, vmin = np.max(ZS), np.min(ZS)\n",
    "    import matplotlib.colors\n",
    "    norm = matplotlib.colors.Normalize(vmax=vmax, vmin=vmin)\n",
    "  \n",
    "    for ax, z in zip(grid2, ZS):\n",
    "        im = ax.imshow(z, norm=norm,\n",
    "                       origin=\"lower\", extent=extent,\n",
    "                       interpolation=\"nearest\")\n",
    "  \n",
    "    # With cbar_mode=\"single\", cax attribute of all axes are identical.\n",
    "    ax.cax.colorbar(im)\n",
    "    ax.cax.toggle_label(True)\n",
    "  \n",
    "    for ax, im_title in zip(grid2, [\"(a)\", \"(b)\", \"(c)\"]):\n",
    "        t = add_inner_title(ax, im_title, loc=2)\n",
    "        t.patch.set_ec(\"none\")\n",
    "        t.patch.set_alpha(0.5)\n",
    "  \n",
    "    grid2[0].set_xticks([-2, 0])\n",
    "    grid2[0].set_yticks([-2, 0, 2])\n",
    "  \n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "150\n",
      "75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGbNJREFUeJzt3X2QHdWZ3/HvTzMa0DsgIcxK2vAirZB4kyUZnNjFYjBY\nwJawDWWDbYLXirGrwq4TXCE43oIUSbm8L7Vrb5mQkDWBJV4pRFkb1a6Wl2IhVFxgkMAYiTcJEEji\nRUKwAvGiedGTP+YK3RmN5p6e6Zm+c+7vUzWle7uPTp9z++iZVt8+51FEYGZmeRlXdQPMzKx8Du5m\nZhlycDczy5CDu5lZhhzczcwy5OBuZpahhsFd0q2SdkjacIj9kvSXkjZL+o2kxeU308zMiki5cr8N\nWDbI/guAebWfq4Cbh98sMzMbjobBPSIeAt4apMjFwF9Hr0eAIyQdW1YDzcysuPYS6pgFbK17v622\n7bX+BSVdRe/VPZMmTVpy0kknlXD4aqxfv/7NiDi6//ZW6CO0Rj9boY+QTz9boY8weD/7iIiGP8Bx\nwIZD7Ps74NN17+8Hljaqc8mSJTGWAevCfWyZfrZCH2OM97MV+hiR3s8ynpbZDsypez+7ts3MzCpS\nRnBfA/zL2lMznwR2R8RBt2TMzGz0NLznLmklcDYwQ9I24AZgPEBE/FdgLXAhsBl4H/j9kWqsmZml\naRjcI+LyBvsD+NeltcjMzIbNM1TNzDLk4G5mliEHdzOzDDm4m5llyMHdzCxDDu5mZhlycDczy5CD\nu5lZhhzczcwy5OBuZpYhB3czsww5uJuZZcjB3cwsQw7uZmYZcnA3M8uQg7uZWYYc3M3MMuTgbmaW\nIQd3M7MMObibmWXIwd3MLEMO7mZmGXJwNzPLkCKikgO3TZkU7dOPrOTYZt273qbn3fcAkLQ+Ipbu\n3+exOXZM2fMhb775JtA657Hz5e3dETG+Ubn20WjMgAeefiQf+6M/LLdSlVvdkKiaX5YfiRH4ECru\n0kh4/T//5SH3ZTk2qx6XMCJj8+ib7zzkvhE5j03glW/++86UcoVvy0i6VdIOSRsG2PddSSFpRtF6\nzcysPEO5534bsKz/RklzgPOBV4bZJjMzG6bCwT0iHgLeGmDXXwDXkuV/4s3MxpZSnpaRdDGwPSKe\nbFDuKknrJK3b/2WWWTPw2MyDz+MBww7ukiYC/wG4vlHZiLglIpZGxNK2KZOGe2iz0nhs5sHn8YAy\nrtxPBI4HnpS0BZgNPC7pYyXUbWZmQzDsRyEj4ilg5v73tQC/NCLeHG7dZmY2NEN5FHIl8DAwX9I2\nSSvKb5aZmQ1H4Sv3iLi8wf7jhtya0VRkUkeRuRepZYscv8jkj30F6t1X8YSnqif2NKuRGJuFxnDF\nY3MkxmUjGY5bry1jZpahpOAuaZmk5yRtlnTdAPt/W9IDkp6Q9BtJF5bfVDMzS9UwuEtqA24CLgAW\nApdLWtiv2B8Bd0bEx4HLgP9SdkPNzCxdypX7GcDmiHgxIjqBVcDF/coEMLX2ehrwanlNNDOzolKC\n+yxga937bbVt9f4j8DVJ24C1wB8MVFH97LEhtNXMzBKV9YXq5cBtETEbuBC4Q9JBddfPHivpuGZm\nNoCU4L4dmFP3fnZtW70VwJ0AEfEwcDjgZX/NzCqSEtwfA+ZJOl5SB71fmK7pV+YV4FwASQvoDe47\ny2yomZmlaxjcI6IbuBq4B3iG3qdiNkq6UdLyWrHvAt+U9CSwEvh6VJW/z8zM0maoRsRaer8ord92\nfd3rp4FPldu0ISjy66TIzLqRmN3Xk/51h7oLlN2b3lj1lD/VLtrST0K0J5ZtG4Hjj/alR9Vjs8is\n04rHZmnjske07U6bhD9mxm0BnqFqZpYhB3czsww5uJuZZahQcJd0q6QdkjbUbftTSc/W1pT5uaQj\nym+mmZkVUfTK/TZgWb9t9wGnRMRpwPPA90pol5mZDUOh4B4RDwFv9dt2b+1xSYBH6J3kZGZmFSr7\nnvs3gH841E5nJrdm5bGZhz7n8b3WPo+lBXdJ3we6gZ8dqowzk1uz8tjMQ5/zOKm1z+OwE2QDSPo6\n8HvAuZ6ZamZWvWEHd0nLgGuB342I94ffJDMzG65CwV3SSuBsYEZt7fYb6H065jDgPkkAj0TEtxtW\nFqQnwt2X2L4C05bVnV52XGd62bYP08p2vJNcJRN2pv9naMKu7saFasbtTftgew5Pn0f94VHpZT+Y\nkXZXsPOI9P53TyohCXOGYzN1XEL1YzN1XALsGWRfx7vBrP/bk1TPmBm3BRQK7hFx+QCbf1pSW8zM\nrCSeoWpmliEHdzOzDDm4m5llyMHdzCxDDu5mZhlycDczy5CDu5lZhhzczcwy5OBuZpahUhYOG7LE\nGbepU7fVVWDadqGyyUVp+zCt3OG70qcbT9ucvmRP+3Nbk8v27HqrcSHg8KlTk+uccOKc5LLt86Yk\nlXu3wDVIJBZVoxnumY3N1HEJ1Y/N1HEJwOJD7xq350MmPfRsUjVjZdwW4St3M7MMObibmWWocHA/\nRJLsoyTdJ2lT7c8jy22mmZkVMZQr99s4OEn2dcD9ETEPuL/23szMKlI4uA+UJBu4GLi99vp24PPD\nbJeZmQ1DWffcj4mI12qvXweOGahQn+S1e1o7ea01F4/NPNSfx859BR4RylDpX6jWcqgO+CxVn+S1\nk1s7ea01F4/NPNSfx45xh1fdnEqVFdzfkHQsQO3PHSXVa2ZmQ1BWcF8DXFl7fSVwV0n1mpnZEAzl\nUciVwMPAfEnbJK0AfgicJ2kT8NnaezMzq0jh5QcOkSQb4NxCFQkYlzbNOcanVRkT0jKdA2hiWiZ2\ngMlT0qdYHzXxg6Ryezo7kuvctD192sDEF05KLjt5e9rn37Y3fTp616T0qfN7j0wr2zkt/fg9ExLH\n1GCXNRmOzdRxCdWPzdRxCcCGlw65q2fqBP7pcwuTqhkr47YIz1A1M8tQUnCXtEzSc5I2SxpwgpKk\nL0l6WtJGSX9TbjPNzKyIhrdlJLUBNwHnAduAxyStiYin68rMA74HfCoi3pY0c6QabGZmjaVcuZ8B\nbI6IFyOiE1hF74zUet8EboqItwEiwo9CmplVKCW4zwLqF2LeVttW73eA35H0S0mPSOq/9gzQd/bY\n0JprZmYpyvpCtR2YB5wNXA78d0lH9C9UP3uspOOamdkAUoL7dqA+Tcns2rZ624A1EdEVES8Bz9Mb\n7M3MrAIpwf0xYJ6k4yV1AJfROyO13i/ovWpH0gx6b9O8WGI7zcysgIbBPSK6gauBe4BngDsjYqOk\nGyUtrxW7B9gl6WngAeDfRcSukWq0mZkNLmmGakSsBdb223Z93esArqn9pEu949+eNrtv/IT0TNZz\nZvxTctmLPrahcaGaS6f+JqncsW0Tkut84qRG2ZwP+OvTP51c9sGtc5PKvbdzYnKd6ix/Xly0FZi9\nl3r4RjNQMxubqeMSqh+bqeMSYOIg3e+aGrx2Ttr5GTPjtgDPUDUzy5CDu5lZhkoL7pL+bW3pgQ2S\nVkpq7ZXyzcwqVEpwlzQL+ENgaUScArTR+1SNmZlVoMzbMu3ABEntwETg1RLrNjOzAkoJ7hGxHfgz\n4BXgNWB3RNzbv1yfJMTvOgmxNQ+PzTz4PB5Q1m2ZI+ldTOx44LeASZK+1r9cnyTEU5yE2JqHx2Ye\nfB4PKOu2zGeBlyJiZ0R0AX8L/IuS6jYzs4LKCu6vAJ+UNFGS6E2590xJdZuZWUFl3XP/FbAaeBx4\nqlbvLWXUbWZmxRVOkH0oEXEDcEOhv6Ryk8IqPcct4wocu4f0insSqx2vtuQ6p49LT2485/C3ksse\nPWVPUrnOzvS2dr2fnlyZnsTPtfzcwTQ8pZmNzdRxCdWPzdRxCTDYV6ZqC8ZP25tUz5gZtwV4hqqZ\nWYYc3M3MMuTgbmaWIQd3M7MMObibmWXIwd3MLEMO7mZmGXJwNzPLkIO7mVmGHNzNzDJU2vIDQxLl\nTuPt6U6fNr3rvYnJZZ/Y/dvJZdsSG/tb499OrvPVrhOSyz75zuzksrs/SMuEuK8n/XPNRmZjM3Vc\nQvVjM3VcwuABLKJFx26Nr9zNzDLk4G5mlqHSgrukIyStlvSspGck/fOy6jYzs2LKvOf+Y+DuiLhU\nUge9SbLNzKwCpQR3SdOAs4CvA0REJ9BZRt1mZlZcWbdljgd2Av9D0hOS/krSQdlpnZncmpXHZh58\nHg8oK7i3A4uBmyPi4/QmSLmufyFnJrdm5bGZB5/HA8oK7tuAbbVcqtCbT3VxSXWbmVlBZSXIfh3Y\nKml+bdO5wNNl1G1mZsWV+bTMHwA/qz0p8yLw+yXWbWZmBZQW3CPi18DSsurrY19isa70/4jseS99\nivOzzEwuu/PDyUnlJrR3Jdf5Qff45LJFpq6nfgZFPtfUc5WNMTI2U8clVD82i/T/iMF2htI/9wzH\nrWeompllyMHdzCxDScFd0jJJz0naLOmgRxzryl0iKSSNzO0ZMzNL0jC4S2oDbgIuABYCl0taOEC5\nKcB3gF/132dmZqMr5cr9DGBzRLxYW1ZgFXDxAOX+E/DHwIclts/MzIYgJbjPArbWvd9W2/YRSYuB\nORHx94NVVD81uHBLzcws2bC/UJU0Dvhz4LuNytZPDR7ucc3M7NBSgvt2YE7d+9m1bftNAU4BHpS0\nBfgksMZfqpqZVScluD8GzJN0fG326WXAmv07I2J3RMyIiOMi4jjgEWB5RPjWi5lZRRrOUI2IbklX\nA/cAbcCtEbFR0o3AuohYM3gNJUhMVhw96VV2702fnPvOvvRZn+99cFhSuXHj0hMW79uXmKyZYomY\nU2fvRU/68ZMTS+dijIzN1HEJ1Y/NQjOiBxMFxm6G4zZpFEXEWmBtv23XH6Ls2cNvlpmZDYdnqJqZ\nZajU4C6prZaJ6e/KrNfMzIop+8r9O8AzJddpZmYFlRbcJc0GLgL+qqw6zcxsaMq8cv8RcC2DrIzs\n5LXWrDw289DnPO5p7fNYSnCX9HvAjohYP1g5J6+1ZuWxmYc+53Fya5/Hsq7cPwUsr81QXQWcI+l/\nllS3mZkVVFaC7O9FxOzaDNXLgH+MiK+VUbeZmRXn59zNzDJUWoLs/SLiQeDBtMKJlabODC4wFTrS\nZ1jTsy99Sn9PZ9rvSxWZ0V+grYWmUacmBR6pqdlF+jXaMhubqeMSmmBslpmsusDnnqyZx20dX7mb\nmWXIwd3MLEMO7mZmGXJwNzPLkIO7mVmGHNzNzDLk4G5mliEHdzOzDDm4m5llyMHdzCxDpS8/MCLK\nngoOxabUF8hcn9qGEZvBXHUW9zEyNbs0Y2VsFllSIL1oMVWPzcFkOG595W5mlqGyknXMkfSApKcl\nbZT0nTLqNTOzoSnrtkw38N2IeFzSFGC9pPsi4umS6jczswLKStbxWkQ8Xnv9LvAMMKuMus3MrLjS\n77lLOg74OPCrAfY5CbE1JY/NPPg8HlBqcJc0Gfg/wL+JiHf673cSYmtWHpt58Hk8oLTgLmk8vYH9\nZxHxt2XVa2ZmxZX1tIyAnwLPRMSfl1GnmZkNXVlX7p8CrgDOkfTr2s+FJdVtZmYFlfIoZET8P4rN\nwTMzsxE0NpYfSNUUU4g9/d8GUPl5aYJrryo+g8o/9+p4+QEzsww5uJuZZSgpuEtaJuk5SZslXTfA\n/mtq68r8RtL9kv5Z+U01M7NUDYO7pDbgJuACYCFwuaSF/Yo9ASyNiNOA1cCflN1QMzNLl3Llfgaw\nOSJejIhOYBVwcX2BiHggIt6vvX0EmF1uM83MrIiU4D4L2Fr3fhuDLwq2AviHgXbUr/uQ3kQzMyuq\n7LVlvgYsBf50oP316z6UeVwzM+sr5Tn37cCcuveza9v6kPRZ4PvA70bE3nKaZ2ZmQ5Fy5f4YME/S\n8ZI6gMuANfUFJH0c+G/A8ojYUX4zzcysiIZX7hHRLelq4B6gDbg1IjZKuhFYFxFr6L0NMxn4371r\niPFKRCwfwXY3rxaeEWdNzOOy5SQtPxARa4G1/bZdX/f6syW3y8zMhsEzVM3MMlRmso5BZ7Gamdno\nKStZR8osVjMzGyVlXbk3nMVqZmajp6zgnjSL1ZnJrVl5bObB5/GAUf1C1ZnJrVl5bObB5/GAsoJ7\n0ixWMzMbHWUF94azWM3MbPSUlSB7wFmsZdRtZmbFKaKaecnt7e2xaNGiPtt27tzJ0UcfXUl7ilq/\nfv37wDODlZk4ceKSBQsWjKl+1WvQxxnAm3Cgn2PJli1bePPNNwGQtL5+pVJJO4GX68tPnDhxyYwZ\nM8bkeYRBz+VH5xHG3rkseh4BOjo6lpx66qmj0r6R+Le/fv367ogY37BgRFTys2TJkuhvoG3Nit51\ndZL6OJb6VW+wPtbvG6v92y/1XI7lfh6qj/2359jH/j8TJ04ctTaNxOeZ2k8vP2BmliEHdzOzDDVV\ncL/qqquqbsKIyLRft1TdgNHm85iHGTNmjNqxqhwzDu6jIMd+RUTLBQWfxzyM5pfiDu5mZlYqB3cz\nsww1TXC/++67mT9/PnPnzuWHP/xh1c0pxVNPPcWpp57KokWLWLp0aeO/0KQk3Spph6QNdduOknSf\npE3PP/88b7/9dpVNHFGSlm3YsCGrsQkgaYukpyT9WtK6qttTtkY5Jvbu3cuXv/xl5s6dy5lnnsmW\nLVsKH2Pr1q185jOfYeHChZx88sn8+Mc/PqjMgw8+yLRp01i0aBGLFi3ixhtvHFJ/Ckt5XnIkfuqf\n/+zu7o4TTjghXnjhhdi7d2+cdtppsXHjxtKfDy0TCc+adnR0xM6dO6tr5DDt7yNwFrAY2BAHno/+\nE+C6iGDWrFlx7bXXVtvYYRjsXNI74/qFU045ZcyMzYEM1EdgCzAjDvHvcqyp7+P+8wacAHQATwIL\no66PN910U3zrW9+KiIiVK1fGl770pcLHfPXVV2P9+vUREfHOO+/EvHnzDhofDzzwQFx00UXD6Vof\nKbEnmuU590cffZS5c+dywgkn0NHRwWWXXcZdd91VdbOsJiIeAt7qt/li4HaA6dOn84tf/GLU2zVK\nzgA2H3bYYR6bY0vDHBN33XUXV155JQCXXnop999///5fDMmOPfZYFi9eDMCUKVNYsGAB27c3x5qJ\nTRHct2/fzpw5BxaVnD17dtN8QMN1/vnns2TJEm65JbuHEo6JiNcA2tvbeeONN6puz0jpk6sgp7EJ\nBHCvpPWScnsUqGGOifq4097ezrRp09i1a9eQD7hlyxaeeOIJzjzzzIP2Pfzww5x++ulccMEFbNw4\nOstulbJwmA1s/vz5PP744+zYsYPzzjuPk046ibPOOqvqZpVOEpKqboYV9+mI2C5pJnCfpGeXLFlS\ndZvGpD179nDJJZfwox/9iKlTp/bZt3jxYl5++WUmT57M2rVr+fznP8+mTZtGvE1NceU+a9Ystm49\n8Et227ZtzJp1UCKnMaejowOAmTNn8oUvfIFHH3204haV6g1JxwJ0dXUxc+bMqtszUvrkKshlbAJE\nxPbanzuAn9N7KyMXDXNM1Med7u5udu/ezfTp0wsfqKuri0suuYSvfvWrfPGLXzxo/9SpU5k8eTIA\nF154IV1dXR8tdjaSmiK4f+ITn2DTpk289NJLdHZ2smrVKpYvX151s4ZF0qSenh4A3nvvPe69915O\nOeWUiltVqjXAlQC7du3i4ouzTZn7GDBv79692YxN6B2fkqbsfw2cD2wY/G+NKQ1zTCxfvpzbb78d\ngNWrV3POOecU/h9oRLBixQoWLFjANddcM2CZ119//aN7+Y8++ij79u0b0i+Ropritkx7ezs/+clP\n+NznPkdPTw/f+MY3OPnkk6tu1nAd89xzz3H66afT3d3NV77yFZYtW1Z1m4ZE0krgbGCGpG3ADcAP\ngTslrZgyZQrXXXfQk2ZZiFqugk2bNv39ggULchmbAMcAP68Fs3bgbyLi7rH8yG69OESOCUk3nnji\niQCsWLGCK664grlz53LUUUexatWqwsf55S9/yR133PHRI88AP/jBD3jllVcA+Pa3v83q1au5+eab\naW9vZ8KECaxatWpUbmNWtp770qVLY926sftobf+1owfSCn2E1uhnK/QRxnY/W6GPkN7PprgtY2Zm\n5XJwNzPLkIO7mVmGHNzNzDLk4G5mliEHdzOzDDm4m5llyMHdzCxDDu5mZhlKCu6NMppIOkzS/6rt\n/5Wk48puqJmZpWsY3CW1ATcBFwALgcslLexXbAXwdkTMBf4C+OOyG2pmZulSrtwbZjShLisPsBo4\nV17g28ysMimrQg6U0aR/qpGPytRWY9sNTAf6LFpcy/ayP+PL3vqEy2PQ/IE2tkIfoTX62Qp9hKz6\n2Qp9hEH6Wa/hqpCSLgWWRcS/qr2/AjgzIq6uK7OhVmZb7f0LtTKHXJFe0rqUlc2aVUr7W6GPRco1\nK5/L4uWaUSv0EdLbn3JbpmFGk/oyktqBacDQkxGamdmwpAT3hhlNqMvKA1wK/GNUtVC8mZk1vuc+\nWEYTYF1ErAF+CtwhaTPwFr2/ABq5ZRjtbgYp7W+FPhYp16x8LouXa0at0EdIbH9lmZjMzGzkeIaq\nmVmGHNzNzDJUSXBvtJxBM5N0q6QdjZ6TdR+bXyv00308qGxL9BOAiBjVH3q/lH0BOAHoAJ4EFo52\nO4bR/rOAxcAG93Hs9rFV+uk+tl4/9/9UceWespxB04qIh+h9Imgw7uMY0Ar9dB/7aJV+AtXclhlo\nOYNZFbRjJLmP+WiFfrZCH6F1+gn4C1UzsyxVEdxTljMY69zHfLRCP1uhj9A6/QSqCe4pyxmMde5j\nPlqhn63QR2idfvaq6FvfC4Hn6f3m+vtVfwtdsO0rgdeALnrv2a1wH8deH1uln+5ja/YzIrz8gJlZ\njvyFqplZhhzczcwy5OBuZpYhB3czsww5uJuZZcjB3cwsQw7uZmYZ+v8qPNuvaJ0EUgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110bf6c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "\n",
    "def get_demo_image():\n",
    "    import numpy as np\n",
    "    from matplotlib.cbook import get_sample_data\n",
    "    f = get_sample_data(\"axes_grid/bivariate_normal.npy\", asfileobj=False)\n",
    "    z = np.load(f)\n",
    "    # z is a numpy array of 15x15\n",
    "    return z, (-3, 4, -4, 3)\n",
    "\n",
    "F = plt.figure(1, (5.5, 3.5))\n",
    "grid = ImageGrid(F, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(1, 3),\n",
    "                 axes_pad=0.1,  # spacing between things\n",
    "                 add_all=True,\n",
    "                 label_mode=\"L\",\n",
    "                 )\n",
    "\n",
    "Z, extent = get_demo_image()  # demo image\n",
    "\n",
    "im1 = Z\n",
    "im2 = Z[:, :10]\n",
    "im3 = Z[:, 10:]\n",
    "vmin, vmax = Z.min(), Z.max()\n",
    "for i, im in enumerate([im1, im2, im3]):\n",
    "    ax = grid[i]  \n",
    "    print(im.size)q\n",
    "    ax.imshow(im, origin=\"lower\", vmin=vmin,\n",
    "              vmax=vmax, interpolation=\"nearest\")\n",
    "\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can get a net to memorize some stuff, and make a GIF of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, find an ambiguous example and visualize the product of the weights and the example.\n",
    "\n",
    "Train 2 nets, 1 where labels are regular, another where labels for 3 and 5 are swapped, and show it an ambiguous example?\n",
    "\n",
    "For an ambiguous example in an overfitted net: show the weight matrix @ input example + bias.\n",
    "\n",
    "For an ambiguous example in a regularized net: show the weight matrix @ input example + bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316824\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.311471\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.298839\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.317593\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.282021\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.272163\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.291142\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.246593\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.220413\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.205367\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.165900\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.080920\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.102153\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.035287\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.856040\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.735446\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.667757\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.584762\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.507836\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.421516\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.384983\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.334858\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.190686\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.295095\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.144560\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.205742\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.883432\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.876708\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.023776\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.962700\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.972245\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.885311\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.014806\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.080063\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.874921\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.705438\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.015597\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.897703\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.672325\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.746981\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.529512\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.600226\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.738729\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.534410\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.703298\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.590622\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.557624\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.672660\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.786745\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.733676\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.801747\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.935448\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.566646\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.618307\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.587257\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.743204\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.377940\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.662912\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.625860\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.060521\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.578118\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.865471\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.776946\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.668466\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.550725\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.764539\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.444843\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.585253\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.594227\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.691169\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.526449\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.531927\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.485856\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.418468\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.526423\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.641435\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.295579\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.490336\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.359428\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.536008\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.428535\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.550703\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.607795\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.345189\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.455243\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.462161\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.404140\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.739453\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.400706\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.641301\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.377920\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.318131\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.331973\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.653713\n",
      "\n",
      "Test set: Average loss: 0.1973, Accuracy: 9434/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.541453\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.664615\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.433965\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.667281\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.658674\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.368588\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.620800\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.414077\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.370519\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.696032\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.392708\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.402091\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.579487\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.535621\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.306859\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.540847\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.399241\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.426600\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.302772\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.353816\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.300093\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.605692\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.482448\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.393465\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.171918\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.342596\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.504192\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.384127\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.326088\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.365185\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.267687\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.345374\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.346301\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.187425\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.330166\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.402815\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.454350\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.464404\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.237967\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.393040\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.418483\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.410643\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.282446\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.480824\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.213294\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.337600\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.260670\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.373419\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.435606\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.479301\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.311963\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.252928\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.278913\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.262171\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.455969\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.458929\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.489185\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.394926\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.340693\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.209432\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.357741\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.376154\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.656398\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.310803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.330823\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.486814\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.461023\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.346284\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.387090\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.414740\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.391163\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.289653\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.243272\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.368641\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.209355\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.324647\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.434522\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.147051\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.348294\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.304081\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.190448\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.502217\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.309918\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.255384\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.334357\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.431393\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.314779\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.248780\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.231470\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.428167\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.351761\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.286005\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.340217\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.398567\n",
      "\n",
      "Test set: Average loss: 0.1259, Accuracy: 9607/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.276057\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.289488\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.298004\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.339675\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.602084\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.285991\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.399286\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.520832\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.443948\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.289498\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.560089\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.248233\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.337609\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.187499\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.288995\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.289209\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.296749\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.236586\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.255448\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.291901\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.336135\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.656624\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.237128\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.388448\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.376733\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.434021\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.231199\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.363641\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.472382\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.174873\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.246863\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.532941\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.288374\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.254546\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.293218\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.281939\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.526438\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.417100\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.274657\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.351364\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.337697\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.374889\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.302895\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.254863\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.286147\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.370189\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.459750\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.315684\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.175487\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.262875\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.241438\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.576149\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.215651\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.212586\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.137659\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.160911\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.138820\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.238967\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.462336\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.296145\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.338376\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.242909\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.194750\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.177428\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.268658\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.509761\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.243977\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.245975\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.219628\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.257765\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.198442\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.305924\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.177348\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.162816\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.361046\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.304519\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.152100\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.235284\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.344582\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.228913\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.186642\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.275288\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.254807\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.418728\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.229101\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.373805\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.251859\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.271972\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.379613\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.380161\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.255331\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.299642\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.296204\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.274716\n",
      "\n",
      "Test set: Average loss: 0.1015, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.282239\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.203325\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.302614\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.454643\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.332216\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.111256\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.126737\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.273711\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.535828\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.132521\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.301534\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.441610\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.300221\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.377684\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.221127\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.298784\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.412090\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.174302\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.437897\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.217368\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.268579\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.274594\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.199979\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.258680\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.141888\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.417495\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.342360\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.229204\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.293456\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.229955\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.257579\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.271131\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.266579\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.237290\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.294811\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.306810\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.342219\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.191537\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.181629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.271748\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.200790\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.318684\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.215289\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.119326\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.112552\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.257822\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.321074\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.119655\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.136529\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.223867\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.172835\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.306441\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.206827\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.351573\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.240033\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.139522\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.214827\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.348746\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.354393\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.323012\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.280030\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.198983\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.324618\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.248309\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.304650\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.240162\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.251784\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.380671\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.362481\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.396013\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.173858\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.203287\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.387893\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.371403\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.291821\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.186173\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.152275\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.372996\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.362060\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.353379\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.271964\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.231470\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.292333\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.193057\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.340934\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.479522\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.430957\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.257991\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.338102\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.139522\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.118983\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.243427\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.164576\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.335977\n",
      "\n",
      "Test set: Average loss: 0.0821, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.235266\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.319922\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.344599\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.356014\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.175974\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.123230\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.222540\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.346099\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.396176\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.228704\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.204314\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.164326\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.223442\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.339138\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.219635\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.259637\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.168750\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.221426\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.231941\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.200457\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.417128\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.258442\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.200733\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.221684\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.114227\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.209247\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.110903\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.285265\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.095507\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.152367\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.296742\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.222089\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.342823\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.286405\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.097639\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.318049\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.284943\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.195715\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.185053\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.507423\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.224089\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.150657\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.204428\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.358040\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.309244\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.276708\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.162509\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.274149\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.165086\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.209036\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.253546\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.267948\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.265328\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.296338\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.237541\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.197249\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.253763\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.198957\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.218250\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.278265\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.154978\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.160312\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.246556\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.106432\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.218337\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.205249\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.220231\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.282777\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.236496\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.209466\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.131628\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.365722\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.275806\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.284530\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.189431\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.337791\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.142877\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.183191\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.172693\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.286239\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.326931\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.176707\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.102709\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.182445\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.360698\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.175519\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.101176\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.180127\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.135953\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.229097\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.270954\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.211141\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.296757\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.245070\n",
      "\n",
      "Test set: Average loss: 0.0785, Accuracy: 9750/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.279543\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.720036\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.086821\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.267553\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.205649\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.390844\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.358182\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.249399\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.245992\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.136662\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.281961\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.161085\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.385296\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.293741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.131367\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.394623\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.240090\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.254966\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.265752\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.174758\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.140434\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.132501\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.250095\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.166731\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.155925\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.197254\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.143797\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.178002\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.249732\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.324147\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.330739\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.178461\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.211217\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.158600\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.084524\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.194241\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.056972\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.192131\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.130525\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.289028\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.161015\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.288667\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.218787\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.286096\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.200396\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.358723\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.101320\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.201064\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.120794\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.147043\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.237836\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.295325\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.221599\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.181773\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.228975\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.086831\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.373116\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.186659\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.206233\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.135270\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.369349\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.260921\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.111554\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.260091\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.095495\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.120158\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.203106\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.164592\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.357301\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.245767\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.264137\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.210575\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.349348\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.453581\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.167161\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.143349\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.318328\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.042983\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.298531\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.147213\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.109505\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.216223\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.299361\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.131182\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.117109\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.211386\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.229539\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.231341\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.145535\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.190456\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.139225\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.186338\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.395286\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.198562\n",
      "\n",
      "Test set: Average loss: 0.0658, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.297385\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.337917\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.324818\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.146296\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.459954\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.261407\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.158498\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.247813\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.141050\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.085481\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.314876\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.288912\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.213073\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.132339\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.071874\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.508637\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.149443\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.088367\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.197074\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.154133\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.264944\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.152385\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.134128\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.302749\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.139205\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.145842\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.316908\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.314154\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.181669\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.258082\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.292109\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.323600\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.059975\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.195264\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.211666\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.450571\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.234238\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.503858\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.160076\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.191901\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.038341\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.053387\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.445787\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.119780\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.190147\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.211821\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.098792\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.127657\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.195828\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.132116\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.370550\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.204070\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.198637\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.230135\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.270074\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.162602\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.142467\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.128600\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.234558\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.078203\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.184752\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.177062\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.063914\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.178175\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.295256\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.156938\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.262509\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.173637\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.308974\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.304039\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.101038\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.370494\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.169670\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.557630\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.251536\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.104129\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.377168\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.124100\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.345274\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.104408\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.147478\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.235029\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.204344\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.104726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.159490\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.489104\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.322159\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.382911\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.206411\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.227547\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.095158\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.256408\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.215765\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.218335\n",
      "\n",
      "Test set: Average loss: 0.0704, Accuracy: 9759/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.154275\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.119337\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.194410\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.273578\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.313849\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.191937\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.260273\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.204900\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.133042\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.188201\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.348712\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.263320\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.282595\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.175550\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.248007\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.265108\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.342823\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.203253\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.261350\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.107491\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.151807\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.061352\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.171564\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.220316\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.116686\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.471308\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.103252\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.154857\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.081689\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.156738\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.189588\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.335277\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.311088\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.406745\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.130984\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.246020\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.185726\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.182098\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.103440\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.232348\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.143837\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.137227\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.137124\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.105160\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.203440\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.128252\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.181287\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.128170\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.265512\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.148296\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.057599\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.236860\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.369507\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.148622\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.159947\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.215592\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.125543\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.204918\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.270895\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.131784\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.153334\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.192745\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.129738\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.099173\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.135658\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.194957\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.080640\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.273467\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.164724\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.114803\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.106454\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.281107\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.106404\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.372636\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.211715\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.157960\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.273661\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.225798\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.255543\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.151489\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.182535\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.094122\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.173471\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.189033\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.125754\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.185025\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.139362\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.195589\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.113756\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.255024\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.227179\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.221836\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.165974\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.107104\n",
      "\n",
      "Test set: Average loss: 0.0636, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.140208\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.229688\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.156536\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.111029\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.176620\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.266493\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.154903\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.104516\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.174107\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.095695\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.217636\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.440251\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.126589\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.146616\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.114675\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.089593\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.190001\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.244728\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.209998\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.084391\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.187846\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.096026\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.243291\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.348673\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.130101\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.205201\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.114946\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.299978\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.288918\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.241720\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.117212\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.150165\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.372242\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.102403\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.065448\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.297952\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.305050\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.278855\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.095894\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.133889\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.187215\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.216642\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.196353\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.136095\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.145098\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.067431\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.109302\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.223687\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.088567\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.249170\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.259249\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.098995\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.178859\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.250890\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.145987\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.143264\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.182118\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.168610\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.066033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.172132\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.141159\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.150225\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.123301\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.150698\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.233939\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.165905\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.188945\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.265951\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.153173\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.134099\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.101595\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.373220\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.283456\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.165883\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.255367\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.313904\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.134861\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.199200\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.230385\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.147447\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.403690\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.209200\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.111366\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.174951\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.229622\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.298256\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.129238\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.355792\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.159493\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.187396\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.232451\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.157914\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.123338\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.193972\n",
      "\n",
      "Test set: Average loss: 0.0579, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.365417\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.153184\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.131096\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.126303\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.306514\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.081682\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.187442\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.294603\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.258393\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.134378\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.115370\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.244457\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.110162\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.060226\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.186901\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.202407\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.201083\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.199263\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.143091\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.129637\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.129827\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.131266\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.416273\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.265462\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.487395\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.205046\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.165126\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.152419\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.181199\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.357214\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.293299\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.196919\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.132185\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.143189\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.250488\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.169201\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.165711\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.123978\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.199232\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.078787\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.222714\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.198607\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.210221\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.214415\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.281765\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.159774\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.171401\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.144140\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.109230\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.279174\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.076634\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.163982\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.128607\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.095556\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.132626\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.176025\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.274023\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.075609\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.040001\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.091843\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.138432\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.153619\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.352460\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.063747\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.136526\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.409130\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.134128\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.118119\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.224974\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.124758\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.194753\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.104322\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.079763\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.301282\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.166484\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.297522\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.229646\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.086562\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.206376\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.075594\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.174336\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.067665\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.144139\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.117260\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.289693\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.099384\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.133299\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.106182\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.144515\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.137191\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.196952\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.087121\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.315815\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.177089\n",
      "\n",
      "Test set: Average loss: 0.0541, Accuracy: 9824/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LOG_INTERVAL = 10\n",
    "LR = .01\n",
    "MOMENTUM = 0.5\n",
    "NO_CUDA = True\n",
    "SEED = 1\n",
    "TEST_BATCH_SIZE = 1000\n",
    "\n",
    "CUDA = not NO_CUDA and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=TEST_BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LOG_INTERVAL = 10\n",
    "LR = .01\n",
    "MOMENTUM = 0.5\n",
    "NO_CUDA = True\n",
    "SEED = 1\n",
    "TEST_BATCH_SIZE = 1000\n",
    "\n",
    "CUDA = not NO_CUDA and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=TEST_BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
